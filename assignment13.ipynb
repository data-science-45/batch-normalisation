{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a technique used in training artificial neural networks to improve the stability and performance of the model. It aims to mitigate the internal covariate shift problem, which refers to the change in the distribution of network activations due to parameter updates during training.\n",
    "\n",
    "In the context of artificial neural networks, particularly deep neural networks with many layers, the inputs to each layer are affected by the parameters learned in the preceding layers. This can lead to the activations becoming highly correlated and may result in slower training or convergence issues.\n",
    "\n",
    "Batch normalization addresses this by normalizing the inputs to each layer before applying the activation function. The normalization is performed over mini-batches of data during training. The process involves the following steps:\n",
    "\n",
    "Compute Batch Mean and Variance: For each mini-batch, compute the mean and variance of the activations across the batch.\n",
    "\n",
    "Normalize Activations: Subtract the batch mean and divide by the square root of the batch variance. This centers the activations around zero and scales them to have unit variance.\n",
    "\n",
    "Scale and Shift: After normalization, the activations are scaled by a learnable parameter (gamma) and shifted by another learnable parameter (beta). This allows the model to learn the optimal scale and shift for each layer's activations.\n",
    "Batch normalization has several benefits:\n",
    "\n",
    "Faster Training: By normalizing the activations, it helps to stabilize and speed up the training process, allowing for higher learning rates and faster convergence.\n",
    "\n",
    "Reduced Sensitivity to Initialization: Batch normalization reduces the dependency of the model on the choice of initialization parameters, making it easier to train deeper networks.\n",
    "\n",
    "Regularization: Batch normalization acts as a form of regularization by adding noise to the activations, similar to dropout, which can help prevent overfitting.\n",
    "\n",
    "Improved Gradient Flow: Normalizing the activations helps to maintain a more consistent distribution of gradients throughout the network, which can further facilitate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization offers several benefits during the training of artificial neural networks:\n",
    "\n",
    "Stable and Faster Training:\n",
    "\n",
    "By reducing internal covariate shift, batch normalization stabilizes the training process. It ensures that the distribution of inputs to each layer remains more consistent throughout training, which facilitates faster convergence.\n",
    "With batch normalization, the network can use higher learning rates without risk of divergence, leading to faster training overall.\n",
    "Reduced Sensitivity to Initialization:\n",
    "\n",
    "Traditional neural networks can be sensitive to the choice of initial parameter values, which can affect the convergence and performance of the model.\n",
    "Batch normalization reduces this sensitivity, making it easier to train deep networks. It allows for more straightforward initialization strategies, such as initializing weights closer to zero or using random initialization methods like Xavier or He initialization.\n",
    "Improved Gradient Flow:\n",
    "\n",
    "Normalizing activations ensures that gradients propagated backward through the network are more stable and consistent.\n",
    "This helps to alleviate the vanishing or exploding gradient problem, allowing gradients to flow more smoothly through the network during backpropagation.\n",
    "Regularization:\n",
    "\n",
    "Batch normalization acts as a form of regularization by adding noise to the activations.\n",
    "This noise helps prevent overfitting by adding a slight amount of randomness to the network's representations, similar to the effect of dropout regularization.\n",
    "Generalization:\n",
    "\n",
    "Batch normalization can improve the generalization performance of the model by reducing the risk of overfitting.\n",
    "By ensuring that the network learns more robust features and representations, batch normalization can lead to better performance on unseen data.\n",
    "Robustness to Hyperparameters:\n",
    "\n",
    "Batch normalization makes neural networks less sensitive to hyperparameter choices such as learning rate and initialization methods.\n",
    "This robustness simplifies the process of hyperparameter tuning, making it easier to train effective models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple feedforward neural network without batch normalization\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define a simple feedforward neural network with batch normalization\n",
    "class FFNN_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNN_BN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.bn1(torch.relu(self.fc1(x)))\n",
    "        x = self.bn2(torch.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:    # Print every 100 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Function to test the model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Without batch normalization\n",
    "model = FFNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model(model, trainloader, optimizer, criterion)\n",
    "test_model(model, testloader)\n",
    "\n",
    "# With batch normalization\n",
    "model_bn = FFNN_BN()\n",
    "optimizer_bn = optim.SGD(model_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model(model_bn, trainloader, optimizer_bn, criterion)\n",
    "test_model(model_bn, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first loads the MNIST dataset, defines a simple feedforward neural network without batch normalization (FFNN), and trains it. Then, it defines a similar network with batch normalization (FFNN_BN) and trains it as well. Finally, it evaluates both models on the test set and compares their performance.\n",
    "\n",
    "The impact of batch normalization on the training process and performance of the neural network will be observed through the training loss, training accuracy, and test accuracy. Batch normalization typically leads to faster convergence and better generalization performance, especially in deeper networks, by addressing issues like internal covariate shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Modify batch sizes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainloader_batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m testloader_batch_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(testset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train models with different batch sizes\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Without batch normalization\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Modify batch sizes\n",
    "trainloader_batch_size = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader_batch_size = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train models with different batch sizes\n",
    "# Without batch normalization\n",
    "model_batch_size_32 = FFNN()\n",
    "optimizer_batch_size_32 = optim.SGD(model_batch_size_32.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model(model_batch_size_32, trainloader_batch_size, optimizer_batch_size_32, criterion)\n",
    "test_model(model_batch_size_32, testloader_batch_size)\n",
    "\n",
    "# With batch normalization\n",
    "model_bn_batch_size_32 = FFNN_BN()\n",
    "optimizer_bn_batch_size_32 = optim.SGD(model_bn_batch_size_32.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model(model_bn_batch_size_32, trainloader_batch_size, optimizer_bn_batch_size_32, criterion)\n",
    "test_model(model_bn_batch_size_32, testloader_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the above code with different batch sizes (e.g., 32, 64, 128), you can observe how the training dynamics and model performance change. Smaller batch sizes might lead to noisier updates but can help the model converge faster in some cases. On the other hand, larger batch sizes might provide smoother updates but could slow down convergence.\n",
    "\n",
    "Now, let's discuss the advantages and potential limitations of batch normalization in improving the training of neural networks:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Stabilized Training: Batch normalization helps stabilize the training process by reducing internal covariate shift. This allows for higher learning rates and faster convergence.\n",
    "\n",
    "Improved Generalization: Batch normalization acts as a form of regularization, leading to better generalization performance on unseen data. It helps prevent overfitting by adding noise to the activations.\n",
    "\n",
    "Reduced Sensitivity to Initialization: Batch normalization reduces the dependency of the model on the choice of initialization parameters, making it easier to train deeper networks.\n",
    "\n",
    "Robustness to Hyperparameters: Batch normalization makes neural networks less sensitive to hyperparameter choices such as learning rate and initialization methods.\n",
    "\n",
    "Improved Gradient Flow: Normalizing activations helps maintain a more consistent distribution of gradients throughout the network, which facilitates training.\n",
    "\n",
    "Potential Limitations:\n",
    "\n",
    "Increased Computational Cost: Batch normalization adds computational overhead during both training and inference, as it requires additional computations for normalization and the management of additional parameters.\n",
    "\n",
    "Difficulty with Small Batch Sizes: Batch normalization may not perform well with very small batch sizes, as the statistics computed over a small batch might not accurately represent the population statistics.\n",
    "\n",
    "Dependency on Mini-Batch Statistics: During inference, batch normalization relies on estimated statistics computed during training. If the distribution of input data during inference differs significantly from that during training, performance may degrade.\n",
    "\n",
    "Limitation with Recurrent Neural Networks (RNNs): Batch normalization is not as straightforward to apply to RNNs due to their sequential nature and varying sequence lengths. Techniques like layer normalization or instance normalization are often preferred for RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
